---
title: "Classif_loi_melange"
format: html
editor: visual
---

## Libraries

```{r}
library(dplyr)
library(mdsr)
library(FactoMineR)
library(EMCluster)
```

## Données

Chargement et nettpyage des données. Deux entrées posent problème : **Ligne 4 colonne V8** et Ligne **69 colonne V20**, il y a deux "*.*" au lieu d'un seul.

```{r}
dt <- read.csv("data.csv", header = TRUE)
dt
```

On retire la première colonne des index car on n'en a pas besoin pour la suite des analyses.

```{r}
data<- dt[,-1]
data
```

```{r}
data$V8[4]<-27.2197645017588
data$V20[69]<-37.143573590775
data$V8<-as.numeric(data$V8)
data$V20<-as.numeric(data$V20)
data$V8[4]
data$V20[69]
```

On plot les données pour observer leur distribution.

```{r}
dta <- t(data)
matplot(dta,type="l")
```

## K-means 1.0

On teste en premier lieu une classification avec un algorithme K-means pour voir s'il fonctionne.

```{r}
km.out <- kmeans(data, centers = 3, nstart = 20)
km.out$cluster # vecteur spécifiant à quel cluster appartient chaque courbe
length(km.out$cluster)

data$Kmean_1 <- km.out$cluster
```

### Plot des résultats du K-means

On sépare les données selon leur cluster et on les plot.

```{r}
dt_km_1_1 <- t(data[data$Kmean_1 == 1, ]) 
dt_km_1_2 <- t(data[data$Kmean_1 == 2, ]) 
dt_km_1_3 <- t(data[data$Kmean_1 == 3, ])
```

```{r}
matplot(dt_km_1_1,type="l", col="red", main="K-means - Cluster 1")
matplot(dt_km_1_2,type="l", col="blue", main="K-means - Cluster 2")
matplot(dt_km_1_3,type="l", col="black", main="K-means - Cluster 3")
```

A première vue les résultats n'ont pas l'air d'être séparés proprement du tout. Nous allons donc tenter une autre approche.

En observant les données sur le premier plot, on remarque deux points au niveau desquels les tendances générales des courbes s'inversent : autour de **33** et autour de **66**.

```{r}
matplot(dta,type="l")
abline(v = 33, lwd = 2, col = "red")
abline(v = 66, lwd = 2, col = "red")
```

On propose donc de découper chaque courbe en trois parties :

-   **1** à **33**

-   **34** à **66**

-   **67** à **100**

```{r}
seg_1 <- data[, 1:33]
length(seg_1)
seg_2 <- data[, 34:66]
length(seg_2)
seg_3 <- data[, 67:100]
length(seg_3)
```

Ensuite, on va ajuster un modèle de régression linéaire pour chaque partie de courbe et récupérer les paramètres dans les vecteurs **param_seg_1/2/3**.

```{r}
### SEGMENT 1
X <- seq(1:33)
seg_1<- t(seg_1)

param_seg_1 = c()
for (k in 1:150){
  coef <- lm(seg_1[,k]~X)$coefficients[2]
  param_seg_1 = c(param_seg_1,coef)
}

### SEGMENT 2
X <- seq(1:33)
seg_2 <- t(seg_2)

param_seg_2 = c()
for (k in 1:150){
  coef <- lm(seg_2[,k]~X)$coefficients[2]
  param_seg_2 = c(param_seg_2,coef)
}

### SEGMENT 3
X<-seq(1:34)
seg_3<-t(seg_3)

param_seg_3 = c()
for (k in 1:150){
  coef <- lm(seg_3[,k]~X)$coefficients[2]
  param_seg_3 = c(param_seg_3,coef)
}

```

On récupère les vecteurs paramètres et on les concatène dans une seule matrice **param**.

```{r}
param <- cbind(param_seg_1, param_seg_2, param_seg_3)
head(param)
```

On peut ensuite passer les paramètres dans une PCA pour observer leur distribution.

```{r}
PCA(param)
```

On voit que la PCA les sépare nettement en trois groupes.

## K-means 2.0

On effetue ensuite un autre k-means, mais cette fois sur les paramètres des régressions de chaque bout de courbe.

```{r}
km.out <- kmeans(param, centers = 3, nstart = 40)
km.out$cluster # vecteur spécifiant à quel cluster appartient chaque courbe
length(km.out$cluster)
```

### Plot des résultats du K-means

```{r}
data$Kmean_2 <- km.out$cluster
data$Kmean_2
```

```{r}
dt_km_2_1 <- t(data[data$Kmean_2 == 1, ]) 
dt_km_2_2 <- t(data[data$Kmean_2 == 2, ]) 
dt_km_2_3 <- t(data[data$Kmean_2 == 3, ])

```

```{r}
matplot(dt_km_2_1,type="l", col="red", main="K-means - Cluster 1")
matplot(dt_km_2_2,type="l", col="blue", main="K-means - Cluster 2")
matplot(dt_km_2_3,type="l", col="black", main="K-means - Cluster 3")
```

On observe que les résultats sont toujours suspects. Nous allons tester un autre algorithme.

## Algorithme EM

On va essayer de classifier les courbes en utilisant cette fois un algorithme **EM** sur les paramètres des régressions (matrice **param**)

### Package EMCluster

```{r}
ret <- init.EM(param, nclass = 3)
ret.new <- assign.class(param, ret, return.all = FALSE)
str(ret.new)
```

```{r}
ret.new$class

```

```{r}
data$EM <- ret.new$class
```

#### Plot des résultats de l'algorithme EM

```{r}
dt_em_1 <- t(data[data$EM == 1, ])
dt_em_2 <- t(data[data$EM == 2, ])
dt_em_3 <- t(data[data$EM == 3, ])
```

```{r}
matplot(dt_em_1,type="l", col="red", main="EM - Cluster 1")
matplot(dt_em_2,type="l", col="blue", main="EM - Cluster 2")
matplot(dt_em_3,type="l", col="black", main="EM - Cluster 3")
```

### A la main

On modifie le code de la séance dernière pour l'étendre à trois classes et à trois dimensions.

D'abord on définit la fonction pour calculer la densité de probabilité.

```{r}
proba_density <- function(x, mu, Sigma) {
  pb_dens <- (1/sqrt((2*pi)^3*det(Sigma)))*exp(-1/2*t(x-mu)%*%solve(Sigma)%*%(x-mu))
  return(pb_dens)
}
```

Ensuite on calcule la probabilité d'appartenance d'une courbe à une classe.

```{r}
proba_appart <- function(x, mu, Sigma, p) {
  w1 <- proba_density(x, mu = mu[[1]], Sigma = Sigma[[1]]) * p[1]
  w2 <- proba_density(x, mu = mu[[2]], Sigma = Sigma[[2]]) * p[2]
  w3 <- proba_density(x, mu = mu[[3]], Sigma = Sigma[[3]]) * p[3]
  w <- c(w1, w2, w3)
  
  C <- w/sum(w)
  return(C)
}
```

On définit une fonction pour calculer la log-likelihood.

```{r}
log_likelihood <- function(x, mu, Sigma, p) {
  ll <- 0
  
  for (i in 1:nrow(x)) {
    sum_k <- 0
    for (k in 1:length(p)) {
      sum_k <- sum_k + p[k] * proba_density(x[i, ], mu[[k]], Sigma[[k]])
    }
    ll <- ll + log(sum_k)
  }
  return(ll)
}
```

Et enfin, on définit l'algorithme d'EM. La structure générale suit celle de la séance précédente.

```{r}
E_M <- function(x, n_iter = 100, mu_init, Sigma_init, p_init, tol = 1e-6) {
  x <- x
  mu <- mu_init
  Sigma <- Sigma_init
  p <- p_init
  
  K <- length(p)

  w <- matrix(0, nrow = nrow(x), ncol = K)
  
  # Étape E (Expectation) : Initialisation des probabilités d'appartenance
  for (i in 1:nrow(x)) {
    xi <- x[i, ]
    probs <- proba_appart(xi, mu, Sigma, p)
    w[i,] <- probs
  }
  
  # Initialisation de la log-vraisemblance
  ll_old <- log_likelihood(x, mu, Sigma, p)
  
  # Boucle EM
  for (iter in 1:n_iter) {
    
    # Étape M (Maximization) : Mise à jour des paramètres en prenant en compte les proba d'appartenance calculé precedemment
    mu_new <- vector("list", K)
    Sigma_new <- vector("list", K)
    p_new <- numeric(K)
    
    for (k in 1:K) {
      mu_new[[k]] <- colSums(x*w[,k]) / sum(w[,k])
      centered <- sweep(x, 2, mu_new[[k]])
      Sigma_new[[k]] <- t(centered) %*% (centered * w[,k]) / sum(w[,k])
      p_new[k] <- mean(w[,k])
    }
    
    # Affichage des paramètres à chaque itération
    cat(sprintf("Itération %d\n", iter))
    
    # Calcul de la log-vraisemblance
    ll_new <- log_likelihood(x, mu_new, Sigma_new, p_new)
    cat(sprintf("Log-vraisemblance : %.4f\n", ll_new))
    
    # Critère de convergence : variation relative des paramètres
    delta_mu <- max(sapply(1:K, function(k) max(abs(mu_new[[k]] - mu[[k]]))))
    delta_sigma <- max(sapply(1:K, function(k) max(abs(Sigma_new[[k]] - Sigma[[k]]))))
    delta_p <- max(abs(p_new - p))
    
    # et de la log-vraisemblance
    delta_ll <- abs(ll_new - ll_old)
    
    # Si tous les deltas sont inférieurs à la tolérance, on arrête la boucle, l'algorithme a cvg
    if (delta_mu < tol && delta_sigma < tol && delta_p < tol && delta_ll < tol) {
      cat(sprintf("Convergence atteinte à l'itération %d.\n", iter))
      break
    }
    
    # Mise à jour des paramètres et de la log-vraisemblance
    mu <- mu_new
    Sigma <- Sigma_new
    p <- p_new
    ll_old <- ll_new
    
    # Étape E (Expectation) : Mise à jour des probabilités d'appartenance
    for (i in 1:nrow(x)) {
      xi <- x[i, ]
      probs <- proba_appart(xi, mu, Sigma, p)
      w[i,] <- probs
      labels <- apply(w, 1, which.max)
    }
  }
  
  # Retourne les paramètres estimés
  return(list(mu = mu, Sigma = Sigma, p = p, labels = labels))
}
```

On peut maintenant appliquer l'algorithme à nos données. Pour cela on définit des **mu**, **sigma** et **p** initiaux.

```{r}
x = param
mu = list(
  colMeans(param[1:50, ]),
  colMeans(param[51:100, ]),
  colMeans(param[101:150, ])
)
Sigma = list(diag(3) * 1, diag(3) * 1, diag(3) * 1)
p = c(1/3, 1/3, 1/3)

resultats_EM <- E_M(x, mu, Sigma, p, n_iter = 100)
print(resultats_EM)
```

#### Plot des résultats

On peut ensuite plot les résultats, comme précédemment.

```{r}
data$labels <- resultats_EM$labels

h_em_1 <- t(data[data$labels == 1, ])
h_em_2 <- t(data[data$labels == 2, ])
h_em_3 <- t(data[data$labels == 3, ])
```

```{r}
matplot(h_em_1,type="l", col="red", main="Hand EM - Cluster 1")
matplot(h_em_2,type="l", col="blue", main="Hand EM - Cluster 2")
matplot(h_em_3,type="l", col="black", main="Hand EM - Cluster 3")
```

## Comparaison des méthodes

On peut ensuite comparer les résultats des trois classifications en comparant les graphiques :

```{r}
par(mfrow = c(2,3))

matplot(dt_km_1_1,type="l", col="red", main="K-means 1 - Cluster 1")
matplot(dt_km_1_2,type="l", col="blue", main="K-means 1 - Cluster 2")
matplot(dt_km_1_3,type="l", col="black", main="K-means 1 - Cluster 3")

matplot(dt_km_2_1,type="l", col="red", main="K-means 2 - Cluster 1")
matplot(dt_km_2_2,type="l", col="blue", main="K-means 2 - Cluster 2")
matplot(dt_km_2_3,type="l", col="black", main="K-means 2 - Cluster 3")

matplot(dt_em_1,type="l", col="red", main="EM - Cluster 1")
matplot(dt_em_2,type="l", col="blue", main="EM - Cluster 2")
matplot(dt_em_3,type="l", col="black", main="EM - Cluster 3")

matplot(h_em_1,type="l", col="red", main="Hand EM - Cluster 1")
matplot(h_em_2,type="l", col="blue", main="Hand EM - Cluster 2")
matplot(h_em_3,type="l", col="black", main="Hand EM - Cluster 3")

par(mfrow = c(1,1))
```
